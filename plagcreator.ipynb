{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text modes\n",
    "class Text_mode(Enum):\n",
    "    simple = 0\n",
    "    markov = 1\n",
    "    \n",
    "#plag modes\n",
    "class Plag_mode(Enum):\n",
    "    one_to_one = 0\n",
    "    shuffled = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parses a wiki dump textfile\n",
    "return: dictionary with key: title of wiki article and value: text of wiki article as list of words\n",
    "'''\n",
    "def parse_wiki_dump():\n",
    "    source_file = \"dump/clean_dump.xml\"\n",
    "    \n",
    "    text_file = open(source_file, \"r\")\n",
    "    wiki = text_file.read() #whole file in a string\n",
    "    text_file.close()\n",
    "\n",
    "    texts = wiki.split(\"-------------------------------------------------\") #split file at each separator line\n",
    "    texts = [re.sub(r'==.*==','',x) for x in texts] #remove section headings\n",
    "    texts = [re.sub(r'[^\\w]|\\n|[\\s]',' ',x) for x in texts] #remove punctuation chars\n",
    "    #texts = [re.sub(r'\\d+','',x) for x in texts] #remove numbers\n",
    "    texts = [re.sub(r'\\s{2,}',' ',x).strip() for x in texts] #replace multiple space chars with one space char\n",
    "    texts = [x for x in texts if x != ''] #remove empty strings from list\n",
    "    texts = [x.lower() for x in texts]\n",
    "    texts = {texts[i]: texts[i+1].split(' ') for i in range(0, len(texts), 2)} #turn list into a dictionary\n",
    "    return texts\n",
    "\n",
    "wiki_articles = parse_wiki_dump() #parse only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a list of words and dictionary for the markov chain text generator\n",
    "return: tuple with list of words and dictionary\n",
    "'''\n",
    "def make_words_list_and_db():\n",
    "    #concat all texts of all wiki articles as one bisg list of words\n",
    "    words = []\n",
    "    for text in wiki_articles.values():\n",
    "        words.append(text)\n",
    "    words = sum(words, []) #flatten\n",
    "    \n",
    "    #build triples of three succeeding words with a step size of 1\n",
    "    triples = []\n",
    "    if len(words) >= 3:\n",
    "        for i in range(len(words) - 2):\n",
    "            triple = (words[i], words[i + 1], words[i + 2])\n",
    "            triples.append(triple)\n",
    "    \n",
    "    #build a dictionary with a two word key and a list with all succeeding words as the value\n",
    "    db = dict()\n",
    "    for w1, w2, w3 in triples:\n",
    "        key = (w1, w2)\n",
    "        if key in db:\n",
    "            db[key].append(w3)\n",
    "        else:\n",
    "            db[key] = [w3]\n",
    "    return (words, db)\n",
    "\n",
    "words, db = make_words_list_and_db() #make words list and db only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a random text out of a list of words\n",
    "param number_of_texts: number of texts to be generated\n",
    "param min_length: min length of the text (lower limit of a random length)\n",
    "param max_length: max length of the text (upper limit of a random length)\n",
    "return: list of generated texts; a text is represented as list of words\n",
    "'''\n",
    "def text_generator_simple(number_of_texts, min_length, max_length):\n",
    "    source_file = \"wordlist/germanWords.txt\"\n",
    "    \n",
    "    text_file = open(source_file, \"r\")\n",
    "    words = text_file.read().splitlines() #read lines of file to list\n",
    "    text_file.close()\n",
    "    \n",
    "    random_texts = [] #list with texts\n",
    "    for x in range(number_of_texts):\n",
    "        length = random.randrange(min_length, max_length)\n",
    "        text = []\n",
    "        for y in range(length):\n",
    "            text.append(random.choice(words)) #randomly choosing words\n",
    "        random_texts.append(text)\n",
    "    return random_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a random text using markov chain. The generated text looks more natural\n",
    "param number_of_texts: number of texts to be generated\n",
    "param min_length: min length of the text (lower limit of a random length)\n",
    "param max_length: max length of the text (upper limit of a random length)\n",
    "return: list of generated texts; a text is represented as list of words\n",
    "'''\n",
    "def text_generator_markov(number_of_texts, min_length, max_length):    \n",
    "    random_texts = []\n",
    "    for x in range(number_of_texts):\n",
    "        seed_index = random.randrange(0, len(words) - 3) #randomly choose the index of the word to start with (seed)\n",
    "        w1, w2 = words[seed_index], words[seed_index + 1] #get this word and the next word from the dictionary\n",
    "        text = []\n",
    "        length = random.randrange(min_length, max_length)\n",
    "        for i in range(length):\n",
    "            text.append(w1)\n",
    "            w1, w2 = w2, random.choice(db[(w1, w2)]) #randomly choose one possible word for the selected key\n",
    "        text.append(w2)\n",
    "        random_texts.append(text)\n",
    "    \n",
    "    return random_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly chooses a text part out of a wiki article\n",
    "param length: length of the plagiarized text part\n",
    "return: tuple (title of article, text part as list of words)\n",
    "'''\n",
    "def get_plag_text(plag_mode, length):\n",
    "    article_title = random.choice(list(wiki_articles.keys())) #randomly choose a wiki article\n",
    "    while len(wiki_articles[article_title]) < length:\n",
    "        article_title = random.choice(list(wiki_articles.keys())) #randomly choose a wiki article\n",
    "        \n",
    "    start = random.randrange(0,len(wiki_articles[article_title]) - length) #randomly choose start position of plag\n",
    "    \n",
    "    plag = wiki_articles[article_title][start : start + length] #cut text part out\n",
    "    if plag_mode == Plag_mode.shuffled:\n",
    "            random.shuffle(plag)\n",
    "    \n",
    "    return (article_title, plag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generates texts with embedded plagiarism + info file for each text and outputs them to txt files\n",
    "param number_of_texts: number of texts to be generated\n",
    "param min_text_length: min length of the surrounding text (lower limit of a random length)\n",
    "param max_text_length: max length of the surrounding text (upper limit of a random length)\n",
    "param plag_length: length of the plagiarized text part\n",
    "param output_dir: output directory for the generated texts\n",
    "'''\n",
    "def generate_plags(text_mode, plag_mode, number_of_texts, min_text_length, max_text_length, plag_length, output_dir, ):\n",
    "    if text_mode == Text_mode.simple:\n",
    "        random_texts = text_generator_simple(number_of_texts, min_text_length, max_text_length)\n",
    "    elif text_mode == Text_mode.markov:\n",
    "        random_texts = text_generator_markov(number_of_texts, min_text_length, max_text_length)\n",
    "    else:\n",
    "        print(\"NO SUCH TEXT MODE (\" + str(text_mode) + \")!\")\n",
    "        return\n",
    "    \n",
    "    plag_texts = []\n",
    "    \n",
    "    i = 0 #index for file names\n",
    "    for text in random_texts:\n",
    "        plag_start = random.randrange(0, len(text)-1) #position of plag in surrounding text\n",
    "        plag = get_plag_text(plag_mode, plag_length) #randomly choose plag\n",
    "    \n",
    "        text[plag_start : plag_start] = plag[1] #insert plag into surrounding text\n",
    "        plag_text = ' '.join(text) #convert list of words int space separated string\n",
    "        \n",
    "        #print info\n",
    "        print(\"text_length: \" + str(len(text)))\n",
    "        print(\"plagiarized_article: \" + plag[0])\n",
    "        print(\"plagiarism_start: \" + str(plag_start))\n",
    "        print(\"plagiarism_end: \" + str(plag_start + len(plag[1])-1))\n",
    "        print(\"plagiarism_length: \" + str(plag_length))\n",
    "        print(\"plagiarism_text_part:\\n\" + ' '.join(plag[1]))\n",
    "        print(\"text_with_plagiarism:\\n\" + plag_text)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        #write text to file\n",
    "        output_file_name = output_dir + \"/plag\" + str(i) +\".txt\"\n",
    "        output_file = open(output_file_name, \"w\")\n",
    "        output_file.write(plag_text)\n",
    "        output_file.close()\n",
    "        \n",
    "        #write info file\n",
    "        output_file_name = output_dir + \"/plag\" + str(i) +\"_info.txt\"\n",
    "        output_file = open(output_file_name, \"w\")\n",
    "        output_file.write(\"text_length: \" + str(len(text)) + \"\\n\")\n",
    "        output_file.write(\"plagiarized_article: \" + plag[0] + \"\\n\")\n",
    "        output_file.write(\"plagiarism_start: \" + str(plag_start) + \"\\n\")\n",
    "        output_file.write(\"plagiarism_end: \" + str(plag_start + len(plag[1])) + \"\\n\")\n",
    "        output_file.write(\"plagiarism_length: \" + str(plag_length) + \"\\n\")\n",
    "        output_file.write(\"plagiarism_text_part: \" + ' '.join(plag[1]))\n",
    "        output_file.close()\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_length: 240\n",
      "plagiarized_article: andrássy universität budapest\n",
      "plagiarism_start: 145\n",
      "plagiarism_end: 164\n",
      "plagiarism_length: 20\n",
      "plagiarism_text_part:\n",
      "deshalb und im wird exzellenzförderung 16 rahmen kiválósági aub unterstützt die von der ministeriums 2013 darf im zeitraum támogatás des\n",
      "text_with_plagiarism:\n",
      "allem an den koffer kommt muss er das amt auf eigenen wunsch nieder und lebte während der letzten jahrzehnte hat zahlreiche bezüge in nietzsches lehren übermensch an einen sonnenkalender der mechanismus von antikythera war ein deutsch polnischer jurist und gutsherr aus dem späten hellenismus das gerät ein und übernahm die regierung des herzogtums nassau nach vielen komplikationen werden beide der europäischen chemischen industrieverbands es dient zudem der eigenen website bewarb durch die fangraten der hudson s minuet d dur d dur es dur und im dritten reich meisenheim am glan 1969 hubert walbaum otto philipp braun der großmarschall vom schwarzen berge ein deutsches volksbuch geschichte von androklus zieht der löwe bis heute bemerkbar nach seinem gegenstand im trümmerfeld sucht der schneeleopard nie durch die zahlreichen und vielbeachteten werken 1953 wurde vom pakistanischen parlament zu einer auseinandersetzung zwischen händel und seine jugendliebe nun endlich heiraten wollen deshalb und im wird exzellenzförderung 16 rahmen kiválósági aub unterstützt die von der ministeriums 2013 darf im zeitraum támogatás des was ihm seine ausbildung in larroque und eröffnete dort eine druckerei erweiterten buchverlag und druckerei bernhard meyer am 19 juli 1802 wurde mit zahlreichen abbildungen und brasilianischen und deutschen flüchtlingen teilgenommen spätestens im juni 1829 per dekret den verwaltungsbezirk comandancia politica y militar en la españa moderna uned madrid 1991 isbn 84 7926 402 0 silvana ceschi und reto stamm la reina einem stadtteil von warschau kam bei einem onkel in hagen wo er a\n",
      "\n",
      "\n",
      "text_length: 308\n",
      "plagiarized_article: franz beckenbauer\n",
      "plagiarism_start: 9\n",
      "plagiarism_end: 28\n",
      "plagiarism_length: 20\n",
      "plagiarism_text_part:\n",
      "ein sandmann einer er aus bayern lebensgefährtin sekretärin war beziehung fc münchen dieser ging mit beckenbauers des weiteres liiert danach\n",
      "text_with_plagiarism:\n",
      "margot honecker durch ihre aufnahmen argentinischer stars so fotografierte ein sandmann einer er aus bayern lebensgefährtin sekretärin war beziehung fc münchen dieser ging mit beckenbauers des weiteres liiert danach sie unter umständen auch darüber berichtet runck in seinen briefen berichtete bischopinck ausführlich über la comunidad de alemanes de paso flores provinz río negro argentinien einer kleinen gruppe nach den dreharbeiten zu dem typisch gebogenen deltoidgrat der meisten wissenschaftler bis zu 75 pfennige leisteten die drei schnellsten schweizer computer sind stand november 2014 allgemeine freigabe angekündigt für april 2015 2015 freigabe von allgemeiner software ist im südwestlichen china asiatische goldkatzen kommen in bangladesch indien und pakistan bis sich der ausgetretenen anzunehmen im jahre 1848 corpszeitung der teutonia nr 63 göttingen 1937 h gideon berent schwineköper zur geschichte aus d neugriech von paul kruger petrus jacobus joubert und marthinus wessel pretorius von 1880 bis 1883 in paris nieder die universität basel schweiz statt und stellt die semantische ebene ansätze aus der illegalität zu sorgen in den mittelpunkt stellt information ist die europäische integration und die forderungen erfüllen auf die sage mit dem des sekretärs des p e p deraniyagala im jahr 1993 verbot china den binnenhandel mit tigerknochen gewarnt im jahr 1871 felis euptilura zu beschreiben das leben in zoologischen gärten in berlin sein wichtigstes buch das jesus video d 2002 regie sebastian niemann 3 h krcmar informationsmanagement 5 auflage wiley vch weinheim 2007 isbn 978 3831109869 literatur von und zu geringe anlagenkapazität es fehlte an kapital im jahr 2008 fand das buch im auftrag des osdl in vollzeit um die änderungen in den geschützten gebieten danum valley naturschutzgebiet und tabin wildreservat sabahs untersuchte bengalkatzen hatten im gesamten gebiet gerade nur 5 priester eine gelbfieber epidemie überlebt die mission war personell und finanziell ausgezehrt der oberhirte 1954 in liechtenstein war ein bekannter wissenschaftler auf dem zweiten weltkrieg für die des\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_plags(Text_mode.markov, Plag_mode.shuffled, 2, 200, 300, 20, \"plag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#ggf. Zahlen rausfiltern (mit Sascha besprechen)\n",
    "#feste Leerzeichen (mit Sascha besprechen)\n",
    "\n",
    "#mehrere Plagiate in einem Text\n",
    "#nicht nur 1 zu 1 Plagiate, sondern auch z.B. Wörter getauscht oder ersetzt\n",
    "#Plagiate auch in zufälliger Länge (min, max)\n",
    "#Text aus PDF parsen\n",
    "    #darin Plagiat einbauen"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}